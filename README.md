# Drought_Project

# DU Data Analysis Project 4 - Analyzing Drought Prediction in the United States
Team Members: Carolyn Chu, Amy Paschal, Austin McClain, Romario Leal, and David Rodgers
## Overview
We chose the project category: creating an analysis of existing data to make a prediction, classification or regression.
For the data set, we chose the “Predict Droughts using Weather & Soil Data” dataset available on Kaggle to explore the accuracy of drought prediction in the United States in addition to exploring the various features that contribute to a drought.
Ultimately we used a Random Forest Classifier, which gave us an accuracy of 0.7591 and produced feature importance information.
## Data Set
Predict Droughts using Weather & Soil Data
https://www.kaggle.com/datasets/cdminix/us-drought-meteorological-data

The drought data is divided between four spreadsheets:
Soil_data.csv contains the soil data obtained from 3,109 locations
Train_timeseries.csv, validation_timeseries.csv, and test_timeseries.csv all contain meteorological records for each location that exists in the soil_data.csv with daily sampling over the course of 2 decades

Original datasets are located here: “\Data”

## Tools Used
* PySpark - because of large data set
* PySpark.ML
* PySpark.SQL
* Tableau - used for interactive map visualization
* Python Pandas
* Python matplotlib.pyplot
* Python geopy

## Data Cleanup
The vast majority of the weather sample  data points (20,435,100) contain a null score, meaning no drought. The drought scale is an integer scale, however many data readings contained fractional scores.
To clean the data (and reduce the data set size), we removed the null data and rounded the scores to the nearest integer. Rounding the scores reduced the number of bins.

## Model Development
Model iterations and script to test the model located here: “\ML Modeling”

Saved model is located here: “\Trained_Models”

Within the ML Modeling folder there are six iterations of the model along with a script to test the model. We started with developing the models using the test data because it was a smaller dataset to work with. Later, we switched to running it on the training data. At the end, we ran the final model on the test data and saved it to be run independently of the training code.

We began with the Multilayer Perceptron Classifier (v1) which required us to round the scores (target label) to bin the data. The model kept resulting in errors and instead of troubleshooting, we decided to switch to a regression model because the score column was continuous anyway (this way we wouldn’t have to round the score column).

The second model we attempted, the Linear Regression Model (v2), seemed like it could be a good fit initially. We even ran it again using the training data (v3). However, we decided we wanted feature importance which indicated that a decision tree might be the best way to produce that.

The third model, the Decision Tree Regressor (v4), gave us the feature importance we were looking for but resulted in a very low R2 score of -0.0046.

We tried a fourth model, the Random Forest Regressor (v5), to see if it would give us a higher R2 score but the results were almost identical.

After reviewing the notes by the dataset’s author on kaggle, we discovered the training model was never meant to be run on the soil data combined with the meteorological data (time series). We also learned that the author went with binning the score data as we had originally attempted to do. With these two pieces of information, we reverted back to a classification model and landed with the Random Forest Classifier (v6). This new model gave us a decent accuracy of 0.7591 in addition to producing feature importance.


## Directory Structure and Code Files:
  * **Data directory**
    * Contains the data generated by our code files
    * The drought dat- set files are too large to upload to github. They can be found on google drive at: https://drive.google.com/drive/folders/1pzUxwjUi29zp09ljeeVKjA9KBJL9WkzG?usp=sharing


  * **data_analysis.ipynb**
    * Pyspark DataFrames
    * Loads the datasets using Spark
    * Merges tables
    * Creates parquets to speed up queries
    * Uses SQL queries for visualizations
    * GeoPy
      * Gets the location of the FIPS location using latitude and longitude
    * Linear Regressions
    * Uses Spark/SQL queries to create visualizations
    * Uses matplotlib for visualizations
      * results saved to Plot PNG directory
    * Time Series Plots
    * Uses Spark/SQL queries to create visualizations
    * Uses matplotlib for visualizations
      * results saved to Plot PNG directory
  * Plot PNG
    * Directory where all plots from the data_analyisis file are saved to

* **Importance_pie.ipynb**
  * Used Pandas and Matplotlib.plotly
  * Determining the importance of the features from the Time Series data


* **map_data_colab.ipynb**
  * Cleaning: nulls are removed and score data is rounded to reduce bins
  * Data size reduction for visualization. Generates the following files  for use in tableau interactive map:
    * a csv file with the max monthly score data for the decade, 2000-2010
    * a csv file with the max yearly score for the two decades of data 2000-2020
    * a csv file containing the sampling locations (fips, lat, lon)
  * Uses Pyspark and Pyspark.SQL
  * The Interactive Score Map can be found on Tableau public at: https://public.tableau.com/app/profile/amy.paschal/viz/DroughtDataAnnual/AnnualDroughtScores?publish=yes. Press the right arrow to play the animation.


## Resources Used
Used these two links for understanding how to bring CSV files into Google Colab:
https://saturncloud.io/blog/how-to-import-files-from-google-drive-to-colab/
https://stackoverflow.com/questions/56611698/pandas-how-to-read-csv-file-from-google-drive-public

Bringing data into PySpark:
https://docs.cloudera.com/documentation/enterprise/latest/topics/spark_external_storage.html#:~:text=Spark%20can%20access%20all%20storage,compression%20of%20all%20supported%20files.

Interactive map creation: https://www.youtube.com/watch?v=7-sVGqwJyQ8

